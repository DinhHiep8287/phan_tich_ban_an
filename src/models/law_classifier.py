"""
M√¥ h√¨nh demo ƒë∆°n gi·∫£n ƒë·ªÉ g√°n nh√£n ƒëi·ªÅu lu·∫≠t cho b·∫£n √°n
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import pickle
import os
from typing import List, Dict, Tuple, Optional
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from data_loader import DataLoader

class LawClassifier:
    """M√¥ h√¨nh ph√¢n lo·∫°i ƒëi·ªÅu lu·∫≠t cho b·∫£n √°n"""
    
    def __init__(self, model_type: str = 'naive_bayes'):
        self.model_type = model_type
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            stop_words=None,
            min_df=2,
            max_df=0.95
        )
        self.classifier = None
        self.label_encoder = LabelEncoder()
        self.is_trained = False
        
    def prepare_data(self, loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:
        """
        Chu·∫©n b·ªã d·ªØ li·ªáu cho training
        
        Args:
            loader: DataLoader instance
            
        Returns:
            Tuple (X, y) - features v√† labels
        """
        print("üìä Chu·∫©n b·ªã d·ªØ li·ªáu training...")
        
        # L·∫•y d·ªØ li·ªáu b·∫£n √°n v·ªõi ƒëi·ªÅu lu·∫≠t
        cases_with_laws = loader.get_case_with_laws()
        
        if len(cases_with_laws) == 0:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ training")
            return np.array([]), np.array([])
        
        # L·ªçc d·ªØ li·ªáu c√≥ article h·ª£p l·ªá (thay v√¨ law_id)
        valid_data = cases_with_laws.dropna(subset=['article'])
        
        if len(valid_data) == 0:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ training")
            return np.array([]), np.array([])
        
        # Chu·∫©n b·ªã features (text c·ªßa b·∫£n √°n)
        texts = valid_data['text'].fillna('')
        
        # Chu·∫©n b·ªã labels (article thay v√¨ law_id)
        labels = valid_data['article'].astype(str)
        
        print(f"‚úÖ ƒê√£ chu·∫©n b·ªã {len(texts)} samples")
        print(f"üìä S·ªë l∆∞·ª£ng ƒëi·ªÅu lu·∫≠t kh√°c nhau: {labels.nunique()}")
        
        return texts, labels
    
    def train(self, loader: DataLoader, test_size: float = 0.2):
        """
        Hu·∫•n luy·ªán m√¥ h√¨nh
        
        Args:
            loader: DataLoader instance
            test_size: T·ª∑ l·ªá d·ªØ li·ªáu test
        """
        print(f"üöÄ B·∫Øt ƒë·∫ßu training m√¥ h√¨nh {self.model_type}...")
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        texts, labels = self.prepare_data(loader)
        
        if len(texts) == 0:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ training")
            return
        
        # Encode labels
        y_encoded = self.label_encoder.fit_transform(labels)
        
        # Split d·ªØ li·ªáu
        X_train, X_test, y_train, y_test = train_test_split(
            texts, y_encoded, test_size=test_size, random_state=42, stratify=y_encoded
        )
        
        print(f"üìä Training set: {len(X_train)} samples")
        print(f"üìä Test set: {len(X_test)} samples")
        
        # Vectorize text
        print("üî§ ƒêang vectorize text...")
        X_train_vectorized = self.vectorizer.fit_transform(X_train)
        X_test_vectorized = self.vectorizer.transform(X_test)
        
        # Ch·ªçn v√† hu·∫•n luy·ªán classifier
        if self.model_type == 'naive_bayes':
            self.classifier = MultinomialNB()
        elif self.model_type == 'random_forest':
            self.classifier = RandomForestClassifier(n_estimators=100, random_state=42)
        else:
            raise ValueError(f"Model type '{self.model_type}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£")
        
        print(f"üéØ ƒêang training {self.model_type}...")
        self.classifier.fit(X_train_vectorized, y_train)
        
        # ƒê√°nh gi√° m√¥ h√¨nh
        self.evaluate_model(X_test_vectorized, y_test)
        
        self.is_trained = True
        print("‚úÖ Training ho√†n th√†nh!")
    
    def evaluate_model(self, X_test: np.ndarray, y_test: np.ndarray):
        """ƒê√°nh gi√° m√¥ h√¨nh"""
        print("\nüìà ƒê√ÅNH GI√Å M√î H√åNH:")
        print("=" * 50)
        
        # Predict
        y_pred = self.classifier.predict(X_test)
        
        # Accuracy
        accuracy = accuracy_score(y_test, y_pred)
        print(f"üéØ Accuracy: {accuracy:.4f}")
        
        # Classification report - ch·ªâ ƒë·ªãnh labels ƒë·ªÉ tr√°nh l·ªói
        unique_labels = np.unique(np.concatenate([y_test, y_pred]))
        target_names = [self.label_encoder.inverse_transform([label])[0] for label in unique_labels]
        
        print("\nüìä Classification Report:")
        print(classification_report(y_test, y_pred, labels=unique_labels, target_names=target_names))
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        print(f"\nüìã Confusion Matrix Shape: {cm.shape}")
        
        # Top predictions
        print(f"\nüîù Top 10 predictions:")
        unique, counts = np.unique(y_pred, return_counts=True)
        top_predictions = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)[:10]
        
        for i, (pred_class, count) in enumerate(top_predictions, 1):
            article = self.label_encoder.inverse_transform([pred_class])[0]
            print(f"  {i:2d}. Article {article}: {count} predictions")
    
    def predict(self, text: str) -> List[Dict]:
        """
        D·ª± ƒëo√°n ƒëi·ªÅu lu·∫≠t cho m·ªôt b·∫£n √°n
        
        Args:
            text: N·ªôi dung b·∫£n √°n
            
        Returns:
            List c√°c dict ch·ª©a th√¥ng tin d·ª± ƒëo√°n
        """
        if not self.is_trained:
            print("‚ùå M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c training")
            return []
        
        # Vectorize text
        text_vectorized = self.vectorizer.transform([text])
        
        # Predict
        prediction = self.classifier.predict(text_vectorized)[0]
        probabilities = self.classifier.predict_proba(text_vectorized)[0]
        
        # L·∫•y top 5 predictions
        top_indices = np.argsort(probabilities)[::-1][:5]
        
        results = []
        for i, idx in enumerate(top_indices):
            article = self.label_encoder.inverse_transform([idx])[0]
            confidence = probabilities[idx]
            
            results.append({
                'rank': i + 1,
                'article': str(article),
                'confidence': float(confidence),
                'probability': float(confidence)
            })
        
        return results
    
    def predict_batch(self, texts: List[str]) -> List[List[Dict]]:
        """
        D·ª± ƒëo√°n cho nhi·ªÅu b·∫£n √°n
        
        Args:
            texts: List c√°c n·ªôi dung b·∫£n √°n
            
        Returns:
            List c√°c predictions cho t·ª´ng b·∫£n √°n
        """
        if not self.is_trained:
            print("‚ùå M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c training")
            return []
        
        # Vectorize texts
        texts_vectorized = self.vectorizer.transform(texts)
        
        # Predict
        predictions = self.classifier.predict(texts_vectorized)
        probabilities = self.classifier.predict_proba(texts_vectorized)
        
        results = []
        for i, (pred, probs) in enumerate(zip(predictions, probabilities)):
            # L·∫•y top 3 predictions cho m·ªói b·∫£n √°n
            top_indices = np.argsort(probs)[::-1][:3]
            
            sample_results = []
            for j, idx in enumerate(top_indices):
                article = self.label_encoder.inverse_transform([idx])[0]
                confidence = probs[idx]
                
                sample_results.append({
                    'rank': j + 1,
                    'article': str(article),
                    'confidence': float(confidence)
                })
            
            results.append(sample_results)
        
        return results
    
    def save_model(self, filepath: str):
        """L∆∞u m√¥ h√¨nh"""
        if not self.is_trained:
            print("‚ùå M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c training")
            return
        
        model_data = {
            'vectorizer': self.vectorizer,
            'classifier': self.classifier,
            'label_encoder': self.label_encoder,
            'model_type': self.model_type
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh v√†o {filepath}")
    
    def load_model(self, filepath: str):
        """Load m√¥ h√¨nh"""
        if not os.path.exists(filepath):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {filepath}")
            return
        
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.vectorizer = model_data['vectorizer']
        self.classifier = model_data['classifier']
        self.label_encoder = model_data['label_encoder']
        self.model_type = model_data['model_type']
        self.is_trained = True
        
        print(f"‚úÖ ƒê√£ load m√¥ h√¨nh t·ª´ {filepath}")


def demo_classifier():
    """Demo m√¥ h√¨nh classifier"""
    print("üéØ DEMO: M√î H√åNH PH√ÇN LO·∫†I ƒêI·ªÄU LU·∫¨T")
    print("=" * 60)
    
    # Load d·ªØ li·ªáu
    loader = DataLoader()
    data = loader.load_all_data()
    
    if not data:
        print("‚ùå Kh√¥ng th·ªÉ load d·ªØ li·ªáu")
        return
    
    # T·∫°o v√† training m√¥ h√¨nh
    classifier = LawClassifier(model_type='naive_bayes')
    classifier.train(loader)
    
    # Test v·ªõi m·ªôt s·ªë b·∫£n √°n
    print("\nüß™ TEST PREDICTIONS:")
    print("=" * 40)
    
    # L·∫•y m·ªôt s·ªë b·∫£n √°n ƒë·ªÉ test
    test_cases = loader.case_data.head(3)
    
    for i, (_, case) in enumerate(test_cases.iterrows(), 1):
        print(f"\nüìÑ Test case {i}:")
        case_name = case['case_name'][:50] + "..." if len(case['case_name']) > 50 else case['case_name']
        print(f"  T√™n b·∫£n √°n: {case_name}")
        
        # Predict
        predictions = classifier.predict(case['text'])
        
        print(f"  D·ª± ƒëo√°n:")
        for pred in predictions[:3]:  # Top 3
            print(f"    {pred['rank']}. Article {pred['article']}: {pred['confidence']:.3f}")
    
    # L∆∞u m√¥ h√¨nh
    classifier.save_model('models/law_classifier.pkl')
    
    print("\n‚úÖ Demo ho√†n th√†nh!")


if __name__ == "__main__":
    # T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥
    os.makedirs('models', exist_ok=True)
    demo_classifier() 